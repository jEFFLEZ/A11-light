// --- .env first ---
const path = require('node:path');
const { fileURLToPath } = require('node:url');
const fs = require('node:fs');
const dotenv = require('dotenv');

// Prevent DeprecationWarning for util._extend by replacing it early with Object.assign
try {
  const coreUtil = require('util');
  if (coreUtil && typeof coreUtil._extend !== 'undefined') coreUtil._extend = Object.assign;
} catch (e) {
  // ignore
}

// Prefer server-local env (.env.local) for dev, fallback to repo root .env
const localEnvPath = path.resolve(__dirname, '.env.local');
const repoEnvPath = path.resolve(__dirname, '../../.env');
if (fs.existsSync(localEnvPath)) {
  console.log('[A11] Chargement des variables d\'environnement depuis', localEnvPath);
  dotenv.config({ path: localEnvPath });
} else if (fs.existsSync(repoEnvPath)) {
  console.log('[A11] Chargement des variables d\'environnement depuis', repoEnvPath);
  dotenv.config({ path: repoEnvPath });
} else {
  console.log('[A11] Aucun fichier .env trouvé (cherche .env.local puis ../../.env)');
}

// DEBUG: log Nez env vars
console.log('[NEZ ENV] NEZ_TOKENS=', process.env.NEZ_TOKENS);
console.log('[NEZ ENV] NEZ_ADMIN_TOKEN=', process.env.NEZ_ADMIN_TOKEN);
console.log('[NEZ ENV] NEZ_ALLOWED_TOKEN=', process.env.NEZ_ALLOWED_TOKEN);

// Ensure runtime configuration defaults are set to avoid ReferenceErrors
const CTX_SIZE = Number(process.env.CTX_SIZE) || 8192;
const BATCH_SIZE = Number(process.env.BATCH_SIZE) || 4096;
const PARALLEL = Number(process.env.PARALLEL) || 8;
const HOST_BIND = process.env.HOST_SERVER || '127.0.0.1';

// -------------------

// --- Compilation automatique de openai.ts ---
const { execSync } = require('node:child_process');
const tsPath = path.resolve(__dirname, 'providers', 'openai.ts');
const jsPath = path.resolve(__dirname, 'providers', 'openai.js');
try {
  const tsMtime = fs.existsSync(tsPath) ? fs.statSync(tsPath).mtimeMs : 0;
  const jsMtime = fs.existsSync(jsPath) ? fs.statSync(jsPath).mtimeMs : 0;
  if (tsMtime > jsMtime || !fs.existsSync(jsPath)) {
    console.log('[A11] Compilation automatique de openai.ts...');
    execSync(`npx tsc "${tsPath}" --outDir "${path.dirname(tsPath)}"`);
    console.log('[A11] Compilation terminée.');
  }
} catch (e) {
  console.warn('[A11] Erreur compilation openai.ts:', e.message);
}
// -------------------

// Import all required modules at the top
const { spawn } = require("node:child_process");
const express = require("express");
const { Router } = require("express");
const { registerOpenAIRoutes } = require("./src/routes/llm-openai");
const cors = require("cors");
const compression = require("compression");
const { createProxyMiddleware } = require("http-proxy-middleware");
const axios = require("axios");
// OpenAI SDK (CommonJS)
let OpenAI;
try {
  OpenAI = require('openai');
} catch (e) {
  OpenAI = null;
}

const openaiClient = OpenAI ? new OpenAI({
  baseURL: process.env.OPENAI_BASE_URL || (process.env.UPSTREAM_ORIGIN || 'https://api.funesterie.me') + '/v1',
  apiKey: process.env.OPENAI_API_KEY || 'dummy',
  defaultHeaders: {
    'X-NEZ-TOKEN': process.env.NEZ_ALLOWED_TOKEN || process.env.NEZ_TOKENS || 'nez:a11-client-funesterie-pro'
  }
}) : null;
const multer = require("multer");
const open = require("open");
const Tesseract = require('tesseract.js');
const sharp = require('sharp');
const { nezAuth, getNezAccessLog, TOKENS, MODE } = require('./src/middleware/nezAuth');

// qflush supervisor integration disabled to avoid ESM/CJS import issues in some environments
// (Any qflush functionality is intentionally left disabled; enable manually if you install a CommonJS-compatible variant.)
const QFLUSH_AVAILABLE = false;

// === Upload (OCR) - use memory storage to avoid disk writes ===
const upload = multer({
  storage: multer.memoryStorage(),
  limits: { fileSize: 10 * 1024 * 1024, files: 1 }
});
const { WebSocketServer } = require("ws");

const app = express();
const router = Router();

// CORS configuration: allow local dev origins and production origin
const defaultCorsOrigins = ['http://127.0.0.1:3000', 'http://localhost:5173', 'http://localhost:3000', 'https://funesterie.pro', 'https://alphaonze.netlify.app'];
const CORS_ORIGINS = (process.env.CORS_ORIGINS && process.env.CORS_ORIGINS.split(',')) || defaultCorsOrigins;

const corsOptions = {
  origin: function(origin, callback) {
    // Allow requests with no origin (e.g., curl, mobile clients)
    if (!origin) return callback(null, true);
    if (CORS_ORIGINS.indexOf(origin) !== -1) return callback(null, true);
    return callback(new Error('CORS origin denied'));
  },
  credentials: true,
  allowedHeaders: ['Content-Type', 'Authorization', 'X-Requested-With', 'X-NEZ-TOKEN', 'X-NEZ-ADMIN']
};

// Use CORS middleware globally
app.use(cors(corsOptions));
// Ensure preflight requests are handled
app.options('*', cors(corsOptions));

// Ajout express.json AVANT les proxies pour garantir le body POST
app.use(express.json({ limit: "10mb" }));
app.use(express.urlencoded({ extended: true, limit: "10mb" }));

// SUPPRESSION des premiers express.json / express.urlencoded
// app.use(express.json());
// app.use(express.urlencoded({ extended: true }));

registerOpenAIRoutes(router);
// Register TTS routes (piper)
try {
  const registerTTS = require('./routes/tts.cjs');
  if (typeof registerTTS === 'function') registerTTS(router);
} catch (e) {
  console.warn('[Server] Failed to register TTS routes:', e && e.message);
}
app.use("/api", router);
app.use('/api', nezAuth);

// Global runtime flags and port (single source of truth)
let LISTENING = false;
// Ensure PORT is only declared once across modules/contexts.
// Some files in this repository previously declared a top-level `const PORT`,
// which can cause a "Identifier 'PORT' has already been declared" SyntaxError
// when running the server in certain environments. Store the canonical port
// on globalThis and reuse it everywhere.
if (globalThis.__A11_PORT === undefined) {
  globalThis.__A11_PORT = Number(process.env.PORT) || 3000;
}
const PORT = globalThis.__A11_PORT;
console.log('DEBUG: After PORT setup, PORT =', PORT);

// Single DEFAULT_UPSTREAM: prefer LLAMA_BASE if set, otherwise use localhost (11434)
if (globalThis.__A11_DEFAULT_UPSTREAM === undefined) {
  const host = '127.0.0.1';
  // default to llama-server port 8000 when LLAMA_BASE is not set
  const port = process.env.LLAMA_PORT || '8000';
  // If a local LLM router is configured, prefer it as the default upstream
  if (process.env.LLM_ROUTER_URL && process.env.LLM_ROUTER_URL.trim()) {
    globalThis.__A11_DEFAULT_UPSTREAM = process.env.LLM_ROUTER_URL.trim();
    console.log('[Alpha Onze] Using LLM router as DEFAULT_UPSTREAM =', globalThis.__A11_DEFAULT_UPSTREAM);
  } else {
    globalThis.__A11_DEFAULT_UPSTREAM = (process.env.LLAMA_BASE && process.env.LLAMA_BASE.trim()) ? process.env.LLAMA_BASE : `http://${host}:${port}`;
  }
}
const DEFAULT_UPSTREAM = globalThis.__A11_DEFAULT_UPSTREAM;

// Determine backend mode from environment. Defaults to 'local' for LLaMA usage.
// Expose configured backend and LLAMA_BASE for diagnostics.
const LLAMA_BASE_ENV = process.env.LLAMA_BASE && process.env.LLAMA_BASE.trim();
const RAW_BACKEND = String(process.env.BACKEND || '').trim().toLowerCase();
const BACKEND = (LLAMA_BASE_ENV ? 'local' : (RAW_BACKEND || 'local'));
if (LLAMA_BASE_ENV && RAW_BACKEND !== 'local') {
  console.log(`[Alpha Onze] Notice: LLAMA_BASE is set -> forcing BACKEND='local' (was '${RAW_BACKEND || 'unset'}').`);
}
console.log('[Alpha Onze] Effective BACKEND =', BACKEND);

// Normalize system messages into the first user message for models that ignore `system`
function normalizeMessagesForPhi3(messages) {
  try {
    const msgs = Array.isArray(messages) ? messages.filter(Boolean) : [];
    if (!msgs.length) return msgs;
    const systems = msgs
      .filter((m) => m && m.role === 'system')
      .map((m) => String(m.content || '').trim())
      .filter(Boolean);
    const rest = msgs
      .filter((m) => m && m.role !== 'system')
      .map((m) => ({ role: m.role, content: String(m.content ?? '') }));
    if (!systems.length) return rest;

    const sys = systems.join('\n\n');
    const lastUserIdx = (() => {
      for (let i = rest.length - 1; i >= 0; i--) if (rest[i].role === 'user') return i;
      return -1;
    })();
    const firstUserIdx = rest.findIndex((m) => m.role === 'user');
    const targetIdx = lastUserIdx >= 0 ? lastUserIdx : firstUserIdx;
    if (targetIdx === -1) return [{ role: 'user', content: sys }];

    const NO_AUTO = (process.env.A11_NO_AUTO_INTRO || '1') !== '0';
    const q = rest[targetIdx].content || '';
    const isIdentityQuery = /\b(qui\s*es|comment\s*t.*appelles|ton\s*nom|identité|present|présent|who\s*are|your\s*name)\b/i.test(q);

    const alreadyHas = /Alpha\s*Onze|AlphaOnze|Alpha\s*11|Alpha11|Tu\s+es\s+A11/i.test(q);
    if (!alreadyHas && (!NO_AUTO || isIdentityQuery)) {
      const sep = q ? '\n\n' : '';
      rest[targetIdx] = { role: 'user', content: sys + sep + q };
    }
    return rest;
  } catch {
    return messages;
  }
}

const BASE = path.resolve(__dirname);
const LLAMA_DIR = path.join(BASE, "llama.cpp");
const BIN_DIR_REL = path.join("build", "bin", "Release");
const BIN_DIR_FALLBACK = path.join("build", "bin");

// Directory where Piper and its models live
const PIPER_DIR = path.resolve(__dirname, '../../piper');

function findExe() {
  const guesses = [
    path.join(LLAMA_DIR, BIN_DIR_REL, "llama-server.exe"),
    path.join(LLAMA_DIR, BIN_DIR_FALLBACK, "llama-server.exe"),
  ];
  for (const g of guesses) {
    if (require("node:fs").existsSync(g)) return g;
  }
  return null;
}

// Cleanup function for temporary Piper output files
function cleanupTempAudio() {
  try {
    const maxAgeSec = Number(process.env.TTS_MAX_AGE_SEC) || 3600; // 1 hour default
    const keepFiles = [];
    if (!fs.existsSync(PIPER_DIR)) return;
    const files = fs.readdirSync(PIPER_DIR, { withFileTypes: true })
      .filter(d => d.isFile() && /^out_\d+\.wav$/i.test(d.name))
      .map(d => d.name);
    const now = Date.now();
    for (const f of files) {
      const full = path.join(PIPER_DIR, f);
      try {
        const st = fs.statSync(full);
        if ((now - st.mtimeMs) > (maxAgeSec * 1000)) {
          fs.unlinkSync(full);
          console.log('[TTS][Cleanup] removed expired file', full);
        } else {
          keepFiles.push(f);
        }
      } catch (e) {
        console.warn('[TTS][Cleanup] failed to check/remove', full, e && e.message);
      }
    }
    // Optionally log remaining temporary files count
    if (keepFiles.length) console.log('[TTS][Cleanup] kept', keepFiles.length, 'recent temp files');
  } catch (e) {
    console.error('[TTS][Cleanup] error', e && e.message);
  }
}

// Periodic cleanup interval (seconds)
const TTS_CLEANUP_INTERVAL_SEC = Number(process.env.TTS_CLEANUP_INTERVAL_SEC) || 600; // 10 minutes
setInterval(() => {
  try { cleanupTempAudio(); } catch (e) { console.error('[TTS][Cleanup] interval error', e && e.message); }
}, TTS_CLEANUP_INTERVAL_SEC * 1000);

// === Nezlephant Network Filter ===
// Simple network-level filter: checks X-NEZ-TOKEN header against NEZ_ALLOWED_TOKEN env var.
// Allows health checks and nez handshake through so monitoring/debugging still work.
const ALLOWED_NEZ_TOKEN = process.env.NEZ_ALLOWED_TOKEN;
app.use((req, res, next) => {
  try {
    // If Nezlephant explicitly disabled, bypass legacy token check
    if (process.env.NEZ_SECURITY_MODE && String(process.env.NEZ_SECURITY_MODE).toLowerCase() === 'off') {
      return next();
    }

    // If new Nezlephant tokens are configured, prefer the modern middleware (apps/server/src/middleware/nezAuth.js)
    if (process.env.NEZ_TOKENS) return next();

    // Allow health and nez handshake endpoints unconditionally
    if (req.path === '/health' || req.path === '/api/health' || req.path === '/v1/nez/handshake') return next();

    const token = req.header('X-NEZ-TOKEN');

    if (!ALLOWED_NEZ_TOKEN) {
      // No token configured → filter disabled (convenience for dev)
      console.warn('[A11][NEZ] Aucun NEZ_ALLOWED_TOKEN configuré, filtre désactivé.');
      return next();
    }

    if (!token || token !== ALLOWED_NEZ_TOKEN) {
      console.warn('[A11][NEZ] Requête bloquée: mauvais token ou token absent.', { ip: req.ip, path: req.path });
      return res.status(403).json({
        error: 'A11_Nezlephant_Filter',
        message: 'Nezlephant ne t\'a pas encore reconnu sur ce réseau.'
      });
    }

    return next();
  } catch (err) {
    console.error('[A11][NEZ] Middleware failure:', err && err.message);
    return res.status(500).json({ error: 'nez_filter_error', message: String(err && err.message) });
  }
});

// Text-to-Speech endpoint
app.post("/api/tts/speak", async (req, res) => {
  try {
    const { text, voice = 'male', backend = 'auto', provider = 'auto' } = req.body;
    if (!text) {
      return res.status(400).json({ error: "Text required" });
    }

    // Split text into chunks of ≤200 chars
    const chunks = [];
    let i = 0;
    while (i < text.length) {
      chunks.push(text.substring(i, i + 200));
      i += 200;
    }

    // If Piper requested, run piper.exe and return audio
    if (provider === 'piper' || voice === 'fr_FR-siwis-medium') {
      const piperDir = path.resolve(__dirname, '../../piper');
      const piperExe = path.join(piperDir, 'piper.exe');
      const modelsDir = path.join(piperDir, 'models');
      // Prefer model inside piper/models if available, otherwise fall back to piper root
      let modelPath = path.join(piperDir, 'fr_FR-siwis-medium.onnx');
      const candidateModel = path.join(modelsDir, 'fr_FR-siwis-medium.onnx');
      if (fs.existsSync(candidateModel)) modelPath = candidateModel;
      const outputFile = path.join(piperDir, `out_${Date.now()}.wav`);
      const chunkText = chunks.join(' ');

      // Piper args: --model <model> --output_file <output>
      // Allow adjusting speaker pitch via env TTS_SPEAKER_PITCH (default -20 for a deeper voice)
      const pitch = String(process.env.TTS_SPEAKER_PITCH || '-20');
      const args = [
        '--model', modelPath,
        '--output_file', outputFile,
        '--speaker-pitch', pitch
      ];

      console.log('[TTS][Piper] Using model path:', modelPath);

      // Existence checks
      if (!fs.existsSync(piperExe)) {
        console.error('[TTS][Piper] piper.exe not found at', piperExe);
        return res.status(500).json({ error: 'piper_not_installed', path: piperExe });
      }
      if (!fs.existsSync(modelPath)) {
        console.error('[TTS][Piper] model not found at', modelPath);
        return res.status(500).json({ error: 'piper_model_not_found', model: modelPath });
      }

      console.log('[TTS][Piper] Launching piper', { exe: piperExe, args: args.map(a => (typeof a === 'string' && a.length > 200 ? a.slice(0,200)+'...' : a)) });

      // Spawn Piper and capture stdout/stderr for logging
      // Use stdin pipe so we can write the text to the process (piper reads text from stdin)
      const proc = spawn(piperExe, args, { cwd: piperDir, stdio: ['pipe', 'pipe', 'pipe'], windowsHide: true });

      // Write text to piper stdin
      try {
        if (proc.stdin && !proc.stdin.destroyed) {
          proc.stdin.write(chunkText);
          proc.stdin.end();
        }
      } catch (e) {
        console.error('[TTS][Piper] failed to write to stdin', e && e.message);
      }

      let stderrBuf = '';
      let stdoutBuf = '';
      let responded = false;

      proc.stdout && proc.stdout.on('data', (d) => {
        try { stdoutBuf += d.toString(); } catch {};
        // keep concise logs
        const s = String(d).trim();
        if (s) console.log('[TTS][Piper][stdout]', s.length > 300 ? s.slice(0,300)+'...' : s);
      });
      proc.stderr && proc.stderr.on('data', (d) => {
        try { stderrBuf += d.toString(); } catch {};
        const s = String(d).trim();
        if (s) console.error('[TTS][Piper][stderr]', s.length > 300 ? s.slice(0,300)+'...' : s);
      });

      proc.on('error', (err) => {
        if (responded) return;
        responded = true;
        console.error('[TTS][Piper] spawn error', err && err.message, { stderr: stderrBuf.slice(0,1000) });
        return res.status(500).json({ error: 'piper_spawn_error', message: String(err && err.message), stderr: stderrBuf.slice(0,1000) });
      });

      proc.on('close', (code) => {
        if (responded) return;
        responded = true;
        console.log('[TTS][Piper] process exited with code', code);
        if (code !== 0) {
          return res.status(500).json({ error: 'piper_failed', code, stderr: stderrBuf.slice(0,2000) });
        }
        // Read and send the wav file
        fs.readFile(outputFile, (err, data) => {
          if (err) {
            console.error('[TTS][Piper] Failed to read output file', outputFile, err && err.message);
            return res.status(500).json({ error: 'piper_output_read_failed', message: String(err && err.message) });
          }
          res.setHeader('Content-Type', 'audio/wav');
          res.setHeader('Content-Disposition', 'inline; filename="tts.wav"');
          res.send(data);
          // Cleanup
          fs.unlink(outputFile, () => {});
        });
      });
      return;
    }

    // Generate audio for each chunk and concatenate results
    const audioUrls = [];
    for (const chunk of chunks) {
      let audioUrl;
      if (provider === 'elevenlabs' || backend === 'elevenlabs') {
        // ElevenLabs TTS logic (replace with actual API call)
        // audioUrl = await generateElevenLabsTTS(chunk, voice, speed);
        audioUrl = { provider: 'elevenlabs', voice, text: chunk };
      } else if (provider === 'azure' || backend === 'azure') {
        // Azure TTS logic (replace with actual API call)
        // audioUrl = await generateAzureTTS(chunk, voice, speed);
        audioUrl = { provider: 'azure', voice, text: chunk };
      } else if (provider === 'google' || backend === 'google') {
        // Google TTS logic (replace with actual API call)
        // audioUrl = await generateGoogleTTS(chunk, voice, speed);
        audioUrl = { provider: 'google', voice, text: chunk };
      } else {
        // Fallback
        audioUrl = { provider, voice, text: chunk };
      }
      audioUrls.push(audioUrl);
    }
    res.json({ success: true, audioUrls });
  } catch (error) {
    console.error("[TTS] Erreur:", error);
    res.status(500).json({ error: error.message });
  }
});

// TTS streaming endpoint
app.post("/api/tts/stream", async (req, res) => {
  try {
    const { text, voice = 'male', backend = 'openai' } = req.body;
    if (!text) {
      return res.status(400).json({ error: "Text required" });
    }

    console.log(`[TTS] Streaming audio (voice=${voice}): "${text.substring(0, 50)}..."`);
    const audioStream = await tts.streamSpeak(text, voice, backend);

    res.setHeader('Content-Type', 'audio/mpeg');
    res.setHeader('Transfer-Encoding', 'chunked');
    if (audioStream && audioStream.pipe) {
      audioStream.pipe(res);
    } else {
      // Fallback: send a simple JSON if tts backend isn't implemented
      res.json({ success: true, message: 'streaming not available', voice });
    }
  } catch (error) {
    console.error("[TTS] Streaming erreur:", error);
    res.status(500).json({ error: error.message });
  }
});

const sessionsManager = {
  getUserId: async (req) => 'anonymous',
  getHistory: (userId, conversationId) => [],
  listConversations: (userId) => [],
  getUserStats: (userId) => ({ totalMessages: 0, totalConversations: 0 }),
  deleteConversation: (userId, conversationId) => { },
  addMessage: (userId, message, conversationId) => conversationId || 'conv_' + Date.now()
};

// Stub spellChecker to avoid errors
const spellChecker = {
  correctWithInfo: (text) => ({ corrected: text, hasCorrections: false, original: text })
};

// Stub fileCleanup to avoid errors
const fileCleanup = {
  registerUpload: (userId, path, size = 0) => { },
  getUserFiles: (userId) => [],
  getUserStorageUsage: (userId) => 0,
  formatBytes: (bytes) => `${bytes} bytes`,
  getStats: () => ({}),
  cleanupExpiredFiles: () => []
};

// Stub modules to avoid errors
const webSearch = {
  searchAndSummarize: async (query, limit) => ({ results: [], summary: 'Search disabled' })
};

const religiousKB = {
  searchBibleVerse: (ref) => ({ results: [] }),
  categories: {
    christianisme: { books: [], keyTeachings: [], famousVerses: [] },
    bouddhisme: {}
  },
  getBuddhaTeaching: (topic) => ({ topic, teaching: 'Teaching disabled' }),
  getComparison: () => ({ comparison: 'Comparison disabled' })
};

const worldHistory = {
  getTimeline: () => ({ timeline: [] }),
  getUniverseTimeline: () => ({ timeline: [] }),
  getPeriod: (name) => null,
  search: (query) => [],
  getEvolutionTriggers: (q) => []
};

const imageGen = {
  generate: async (prompt, options) => 'image_disabled.png'
};

const animGen = {
  createMorphAnimation: async (paths, options) => 'animation_disabled.gif',
  createSlideshow: async (paths, options) => 'slideshow_disabled.gif',
  createGIF: async (paths, options) => 'gif_disabled.gif',
  listAnimations: () => []
};

// Local mode health check
app.get('/api/health', async (req, res) => {
  const up = await isPortOpen(8000);
  res.json({
    ok: up,
    mode: "local",
    target: "http://127.0.0.1:8000",
    ctx: CTX_SIZE,
    batch: BATCH_SIZE,
    parallel: PARALLEL
  });
});

// Ensure a simple /health root endpoint exists for probes (some tools hit /health)
app.get('/health', async (req, res) => {
  try {
    const llamaUp = await isPortOpen(8000);
    res.json({
      ok: true,
      backend: BACKEND,
      port: PORT,
      llama_up: llamaUp,
      ctx: CTX_SIZE,
      batch: BATCH_SIZE,
      parallel: PARALLEL
    });
  } catch (e) {
    res.status(500).json({ ok: false, error: String(e) });
  }
});

// Lightweight LLM stats endpoint for the UI to avoid 404 spam
app.get('/api/llm/stats', async (req, res) => {
  try {
    const llamaUp = await isPortOpen(8000);
    res.json({
      backend: BACKEND || 'local',
      model: process.env.DEFAULT_MODEL || 'llama3.2:latest',
      gpu: !!process.env.USE_GPU || false,
      lastTps: 0,
      llama_up: llamaUp
    });
  } catch (e) {
    res.status(500).json({ error: String(e) });
  }
});

// ----- Netlify Redirects & Proxies -----
// Ensure backend proxies point to the configured upstream (DEFAULT_UPSTREAM)
// Use LLAMA_BASE when provided, otherwise DEFAULT_UPSTREAM (typically http://127.0.0.1:8000)
{
  const PROXY_TARGET = (process.env.LLAMA_BASE && process.env.LLAMA_BASE.trim()) ? process.env.LLAMA_BASE : DEFAULT_UPSTREAM;
  const IS_OLLAMA = /ollama/i.test(String(PROXY_TARGET)) || /:11434\b/.test(String(PROXY_TARGET));

  if (!IS_OLLAMA) {
    // Only register direct proxy when upstream speaks OpenAI-compatible API
    app.use(
      "/v1",
      createProxyMiddleware({
        target: PROXY_TARGET,
        changeOrigin: true,
        ws: true,
        logLevel: "debug",
        pathRewrite: { "^/v1": "/v1" },
      }),
    );

    app.use(
      "/api/v1",
      createProxyMiddleware({
        target: PROXY_TARGET,
        changeOrigin: true,
        ws: true,
        logLevel: "debug",
        pathRewrite: { "^/api/v1": "/v1" },
      }),
    );
  } else {
    console.log('[Proxy] Detected Ollama upstream; skipping raw /v1 proxy to allow internal translation handlers.');

    // Provide a simple translation layer: OpenAI-style -> Ollama /api/generate
    const registerOllamaBridge = (pathPrefix) => {
      app.post(pathPrefix + '/chat/completions', express.json({ limit: '10mb' }), async (req, res) => {
        try {
          const body = req.body || {};
          const model = body.model || process.env.DEFAULT_MODEL || 'llama3.1:latest';

          // Build a simple prompt from messages if provided, else use body.prompt
          let prompt = '';
          if (Array.isArray(body.messages) && body.messages.length) {
            prompt = body.messages.map(m => (m.role ? `${m.role}: ${String(m.content || '')}` : String(m.content || ''))).join('\n\n');
          } else if (body.prompt) {
            prompt = body.prompt;
          } else if (body.input) {
            prompt = body.input;
          }

          const apiBody = {
            model: model,
            prompt: prompt,
            stream: false
          };

          const r = await axios.post(`${PROXY_TARGET.replace(/\/$/, '')}/api/generate`, apiBody, { timeout: 300000 });
          const llamaData = r.data || {};
          const content = String(llamaData.response || llamaData[0] || '');

          const responseData = {
            choices: [{ message: { role: 'assistant', content } }],
            usage: {
              prompt_tokens: llamaData.prompt_eval_count || 0,
              completion_tokens: llamaData.eval_count || 0,
              total_tokens: (llamaData.prompt_eval_count || 0) + (llamaData.eval_count || 0)
            }
          };

          res.json(responseData);
        } catch (err) {
          console.error('[OllamaBridge] Error forwarding to Ollama:', err && err.message ? err.message : err);
          if (err.response && err.response.data) {
            try { return res.status(err.response.status || 502).json(err.response.data); } catch (e) { /* ignore */ }
          }
          return res.status(502).json({ error: 'ollama_proxy_error', message: String(err && err.message) });
        }
      });
    };

    // Register both /v1 and /api/v1 bridges
    registerOllamaBridge('/v1');
    registerOllamaBridge('/api/v1');
  }
}
// -------------------------------------
// WebSocket bridge stats
app.get("/api/ws/stats", (req, res) => {
  res.json({ enabled: false });
});

LISTENING = true;
app.listen(PORT, '0.0.0.0', () => {
  console.log(`[AlphaOnze] UI (online mode) ready on http://127.0.0.1:${PORT}`);
  // Run initial cleanup at startup
  try { cleanupTempAudio(); } catch (e) { console.error('[TTS][Cleanup] startup error', e && e.message); }
});

console.log('DEBUG: Entering LOCAL MODE else block');
// LOCAL MODE: auto-spawn llama-server and proxy to it (deferred start)
let __LOCAL_MODE__ = true;

// Small net util
const net = require("node:net");
function isPortOpen(port) {
  return new Promise((resolve) => {
    const sock = new net.Socket();
    sock.once("error", () => resolve(false));
    sock.setTimeout(800, () => {
      sock.destroy();
      resolve(false);
    });
    sock.connect(port, "127.0.0.1", () => {
      sock.end();
      resolve(true);
    });
  });
}
console.log('DEBUG: After isPortOpen function definition');

async function launchLlamaServer(forceRestart = false) {
  const exe = findExe();
  if (!exe) {
    console.error(
      String.raw`llama-server.exe introuvable. Lancez .\setup-keykey-llm.ps1 d'abord (dans le dossier projet).`,
    );
    throw new Error("llama-server introuvable");
  }
  const model = findModel();
  if (!model) {
    console.error(
      String.raw`Modèle GGUF introuvable. Lancez .\setup-keykey-llm.ps1 pour en télécharger un.`,
    );
    throw new Error("Modèle introuvable");
  }

  const apiUp = await isPortOpen(8000);
  if (!apiUp || forceRestart) {
    if (serverProc && !serverProc.killed) {
      try { process.kill(serverProc.pid); } catch { }
      await new Promise((r) => setTimeout(r, 300));
    }
    console.log("→ Démarrage de llama-server...");
    const args = [
      "-m",
      model,
      "--ctx-size",
      String(CTX_SIZE),
      "--batch-size",
      String(BATCH_SIZE),
      "--host",
      "127.0.0.1",
      "--port",
      "8000",
      "--parallel",
      String(PARALLEL),
      "--temp",
      "0.7",
    ];
    // Web UI enabled on port 8000 (llama.cpp built-in interface)
    // Add mmproj if present to enable image understanding with vision models
    try {
      const mmproj = findMmproj();
      if (mmproj) {
        console.log(`→ mmproj détecté: ${mmproj}`);
        args.push('--mmproj', mmproj);
      }
    } catch { }
    serverProc = spawn(exe, args, {
      cwd: path.dirname(exe),
      stdio: "ignore",
      windowsHide: true,
      detached: true,
    });
    serverProc.unref();
    console.log("   Attente du démarrage du serveur...");
    await new Promise((r) => setTimeout(r, 3000));
  } else {
    console.log("✔️  llama-server déjà actif sur 127.0.0.1:8000");
  }
}

// LISTENING already declared above; reuse it here
// Note: llama-server n'a pas d'endpoint /v1/reload, donc on désactive cette fonction

async function start() {
  console.log('[Server] start() function called');
  // Check if we should use external LLAMA_BASE or local llama-server
  const useExternalLlama = process.env.LLAMA_BASE && process.env.LLAMA_BASE.trim();
  console.log(`[Server] LLAMA_BASE: "${process.env.LLAMA_BASE}", useExternalLlama: ${useExternalLlama}`);

  if (useExternalLlama) {
    console.log(`[Server] Using external LLAMA_BASE: ${process.env.LLAMA_BASE}`);
  } else {
    // Only launch local llama-server if no external LLAMA_BASE is configured
    console.log('[Server] Launching local llama-server...');
    await launchLlamaServer(false);
  }

  console.log('[Server] About to check LISTENING flag...');
  // Always register local-mode routes, but avoid calling app.listen twice.
  {
    // Use external LLAMA_BASE if set, otherwise use local llama-server
    const TARGET = (process.env.LLAMA_BASE && process.env.LLAMA_BASE.trim()) ? process.env.LLAMA_BASE.trim() : DEFAULT_UPSTREAM;

    // ===== Helper functions for chat completions handler =====

    /**
     * Preprocess messages: identify user, spell check, and manage session
     * @param {Array} msgs - The messages array
     * @param {Object} req - The request object
     * @param {Object} res - The response object
     * @returns {Object} Object containing userId, currentConvId, and processed messages
     */
    const preprocessMessages = async (msgs, req, res) => {
      // 1. Identifier l'utilisateur
      const userId = await sessionsManager.getUserId(req);
      const conversationId = req.headers['x-conversation-id'] || null;
      console.log(`[ChatHandler] User ID: ${userId}, Conversation ID: ${conversationId}`);

      // 2. Corriger l'orthographe du dernier message utilisateur
      if (msgs.length > 0) {
        const lastMsg = msgs[msgs.length - 1];
        if (lastMsg.role === 'user' && lastMsg.content) {
          const correctionResult = spellChecker.correctWithInfo(lastMsg.content);
          if (correctionResult.hasCorrections) {
            console.log(`[SpellCheck] Correction pour ${userId}:`, {
              original: correctionResult.original,
              corrected: correctionResult.corrected
            });
            lastMsg.content = correctionResult.corrected;
            // Optionnel: informer l'utilisateur via un header
            res.setHeader('X-Spell-Corrected', 'true');
            res.setHeader('X-Original-Text', Buffer.from(correctionResult.original).toString('base64'));
          }
        }
      }

      // 3. Sauvegarder le message de l'utilisateur dans la session
      let currentConvId = conversationId;
      if (msgs.length > 0) {
        const userMsg = msgs[msgs.length - 1];
        if (userMsg.role === 'user') {
          currentConvId = sessionsManager.addMessage(userId, {
            role: 'user',
            content: userMsg.content
          }, conversationId);
          // Envoyer l'ID de conversation au client
          res.setHeader('X-Conversation-Id', currentConvId);
        }
      }

      return { userId, currentConvId, messages: msgs };
    };

    /**
     * Prepare API request based on target and message format
     * @param {Object} body - The request body
     * @param {Array} messages - The processed messages
     * @returns {Object} Object containing apiUrl, apiBody, responseType, and isOllama flag
     */
    const prepareApiRequest = (body, messages) => {
      // Default to streaming enabled unless client explicitly sets stream=false
      const isStreaming = !(body.stream === false || body.stream === 'false');

      // Determine target and whether it's Ollama specifically
      const TARGET = (process.env.LLAMA_BASE && process.env.LLAMA_BASE.trim()) ? process.env.LLAMA_BASE.trim() : DEFAULT_UPSTREAM;
      // Ollama seulement si l'URL ressemble à Ollama (port 11434 ou 'ollama' dans l'URL)
      const isOllama = /(:11434\b)|ollama/i.test(String(TARGET));

      // Debug logging to help front-end/backend mismatch diagnosis
      try {
        console.log('[ChatHandler][prepareApiRequest] TARGET=', TARGET, 'isOllama=', isOllama, 'isStreaming=', isStreaming, 'model=', body.model || '(unset)');
      } catch (e) {}

      if (isOllama) {
        // Ollama uses /api/generate with a prompt
        const conversation = messages.map(msg => msg.content).join('\n\n');
        return {
          apiUrl: `${TARGET}/api/generate`,
          apiBody: {
            model: body.model || 'llama3.2:latest',
            prompt: conversation,
            stream: isStreaming
          },
          responseType: isStreaming ? 'stream' : 'json',
          isOllama: true,
          isStreaming
        };
      }

      // Default: treat target as OpenAI-compatible (llama-server)
      return {
        apiUrl: `${TARGET}/v1/chat/completions`,
        apiBody: {
          model: body.model || process.env.DEFAULT_MODEL || 'keykey',
          messages,
          stream: isStreaming
        },
        responseType: isStreaming ? 'stream' : 'json',
        isOllama: false,
        isStreaming
      };
    };

    /**
     * Handle streaming response from API
     * @param {Object} r - The axios response object
     * @param {boolean} isOllama - Whether the target is Ollama
     * @param {string} userId - The user ID
     * @param {string} currentConvId - The conversation ID
     * @param {Object} res - The response object
     */
    const handleStreamingResponse = async (r, isOllama, userId, currentConvId, res) => {
      // Mode streaming: capturer la réponse pour sauvegarder dans la session
      let responseText = '';
      if (isOllama) {
        // Transform Ollama /api/generate streaming to OpenAI streaming format
        res.setHeader('Content-Type', 'text/plain');
        r.data.on('data', chunk => {
          const lines = chunk.toString().split('\n').filter(line => line.trim());
          for (const line of lines) {
            try {
              const data = JSON.parse(line);
              if (data.response) {
                responseText += data.response;
                const openaiChunk = {
                  choices: [{ delta: { content: data.response } }]
                };
                try { res.write(`data: ${JSON.stringify(openaiChunk)}\n\n`); } catch (e) {}
              }
              if (data.done) {
                res.write('data: [DONE]\n\n');
                res.end();
              }
            } catch (e) {
              console.error('[ChatHandler] Error parsing Ollama stream:', e);
            }
          }
        });
      } else {
        // OpenAI-compatible streaming
        r.data.on('data', chunk => {
          responseText += chunk.toString();
        });
        r.data.pipe(res);
      }

      r.data.on('end', () => {
        // Extraire le contenu de la réponse pour le sauvegarder
        if (isOllama) {
          // For Ollama, responseText already contains the content
          if (responseText) {
            sessionsManager.addMessage(userId, {
              role: 'assistant',
              content: responseText
            }, currentConvId);
            console.log(`[Session] Message assistant sauvagardé (Ollama streaming)`);
          }
        } else {
          try {
            const lines = responseText.split('\n').filter(line => line.trim().startsWith('data: '));
            const lastLine = lines[lines.length - 2]; // Avant le [DONE]
            if (lastLine) {
              const data = JSON.parse(lastLine.substring(6));
              const content = data?.choices?.[0]?.delta?.content || data?.choices?.[0]?.message?.content;
              if (content) {
                sessionsManager.addMessage(userId, {
                  role: 'assistant',
                  content
                }, currentConvId);
                console.log(`[Session] Message assistant sauvegardé (streaming)`);
              }
            }
          } catch (err) {
            console.error('[Session] Erreur sauvegarde réponse streaming:', err);
          }
        }
      });
    };

    /**
     * Handle non-streaming response from API
     * @param {Object} r - The axios response object
     * @param {boolean} isOllama - Whether the target is Ollama
     * @param {string} userId - The user ID
     * @param {string} currentConvId - The conversation ID
     * @param {Object} res - The response object
     */
    const handleNonStreamingResponse = async (r, isOllama, userId, currentConvId, res) => {
      // Mode non-streaming
      let responseData;
      if (isOllama) {
        // Transform Ollama /api/generate response to OpenAI format
        const llamaData = r.data;
        responseData = {
          choices: [{
            message: {
              role: 'assistant',
              content: llamaData.response || ''
            }
          }],
          usage: {
            prompt_tokens: llamaData.prompt_eval_count || 0,
            completion_tokens: llamaData.eval_count || 0,
            total_tokens: (llamaData.prompt_eval_count || 0) + (llamaData.eval_count || 0)
          }
        };
      } else {
        responseData = r.data || {};
      }

      const content = responseData?.choices?.[0]?.message?.content;
      if (content) {
        sessionsManager.addMessage(userId, {
          role: 'assistant',
          content
        }, currentConvId);
        console.log(`[Session] Message assistant sauvardé (non-streaming)`);
      }
      res.json(responseData);
    };

    /**
     * Handle chat completion errors with specific error codes
     * @param {Error} e - The error object
     * @param {Object} res - The response object
     */
    const handleChatError = (e, res) => {
      console.error("[/v1/chat/completions] middleware error:", e.message || e);

      // Erreurs spécifiques avec codes plus clairs
      if (e.code === 'ECONNREFUSED') {
        return res.status(503).json({
          error: "Impossible de se connecter au serveur IA local",
          code: "CONNECTION_REFUSED"
        });
      }
      if (e.code === 'ETIMEDOUT' || e.code === 'ESOCKETTIMEDOUT') {
        return res.status(504).json({
          error: "Le serveur IA a mis trop de temps à répondre",
          code: "TIMEOUT"
        });
      }

      res.status(500).json({
        error: String(e?.message || e),
        code: e.code || "UNKNOWN_ERROR"
      });
    };

    /**
     * Check if Llama server is available
     * @param {Object} res - The response object
     * @returns {boolean} True if available, false otherwise (response sent)
     */
    const checkLlamaAvailability = async (res) => {
      try {
        const useExternalLlama = process.env.LLAMA_BASE && process.env.LLAMA_BASE.trim();
        if (useExternalLlama) {
          // For external LLAMA_BASE, test connectivity
          const response = await axios.get(`${process.env.LLAMA_BASE}/api/tags`, { timeout: 5000 });
          return response.status === 200;
        } else {
          // For local llama-server, check if port 8000 is open
          const isUp = await isPortOpen(8000);
          if (!isUp) {
            res.status(503).json({
              error: "Serveur IA local non disponible. Lancez d'abord le serveur Llama.",
              code: "LLAMA_SERVER_UNAVAILABLE"
            });
            return false;
          }
          return true;
        }
      } catch (error) {
        console.error('[checkLlamaAvailability] Error:', error.message);
        res.status(503).json({
          error: "Erreur de connexion au serveur IA",
          code: "LLAMA_CONNECTION_ERROR"
        });
        return false;
      }
    };

    const chatCompletionsHandler = async (req, res) => { 
      try {
        console.log('[ChatHandler] Nouvelle requête de chat');

        // Check Llama server availability
        if (!(await checkLlamaAvailability(res))) {
          return; // Response already sent by checkLlamaAvailability
        }

        const body = req.body || {};
        // Respect explicit body.model or fallback to DEFAULT_MODEL
        body.model = body.model || process.env.DEFAULT_MODEL || 'llama3.2:latest';

        // Map common local alias 'keykey' to the actual GGUF path if configured.
        // This ensures clients sending `model: "keykey"` work without changing client code.
        try {
          const alias = String(body.model || '').trim();
          if (alias === 'keykey' || alias.toLowerCase() === 'keykey') {
            body.model = process.env.DEFAULT_MODEL || 'D:\\A11\\Models\\keykey\\Llama-3.2-3B-Instruct-Q4_K_M.gguf';
            console.log('[ChatHandler] Mapped alias "keykey" to', body.model);
          }
        } catch (err) {
          // ignore mapping errors and continue
        }

        const msgs = Array.isArray(body.messages) ? body.messages : [];
        console.log(`[ChatHandler] Messages reçus: ${msgs.length}`);

        // Preprocess messages: user identification, spell checking, session management
        const { userId, currentConvId, messages: processedMsgs } = await preprocessMessages(msgs, req, res);

        // For testing, skip system prompt injection
        // const ensured = injectSystemPrompt(processedMsgs);
        const ensured = processedMsgs;
        console.log(`[ChatHandler] Messages après injection système: ${ensured.length}`);

        // Respect client-side provider hint (client may send body.provider)
        const clientProvider = (body && body.provider) ? String(body.provider).toLowerCase() : null;
        const isStreamingClient = !(body.stream === false || body.stream === 'false');
        if (clientProvider === 'openai') {
          // Proxy OpenAI requests via backend to avoid frontend CORS issues.
          const upstreamBase = (process.env.OPENAI_BASE_URL || process.env.UPSTREAM_ORIGIN || 'https://api.funesterie.me').replace(/\/$/, '');
          const upstreamUrl = `${upstreamBase}/v1/chat/completions`;
          const nezToken = process.env.NEZ_ALLOWED_TOKEN || process.env.NEZ_TOKENS || 'nez:a11-client-funesterie-pro';

          try {
            console.log('[ChatHandler] OpenAI proxy', { upstreamUrl, streaming: isStreamingClient });

            if (isStreamingClient) {
              // Stream mode: request upstream as stream and pipe through
              const upstreamRes = await axios({
                method: 'post',
                url: upstreamUrl,
                headers: {
                  'Content-Type': 'application/json',
                  'X-NEZ-TOKEN': nezToken
                },
                data: Object.assign({}, body, { messages: ensured }),
                responseType: 'stream',
                timeout: 300000
              });

              // Forward status and content-type
              try { res.status(upstreamRes.status); } catch (e) {}
              const ct = upstreamRes.headers['content-type'];
              if (ct) res.setHeader('Content-Type', ct);
              res.setHeader('Cache-Control', 'no-cache');
              res.setHeader('Connection', 'keep-alive');
              if (res.flushHeaders) try { res.flushHeaders(); } catch (e) {}

              // Pipe upstream stream to client
              upstreamRes.data.on('data', (chunk) => {
                try { res.write(chunk); } catch (e) {}
              });
              upstreamRes.data.on('end', () => {
                try { res.end(); } catch (e) {}
              });
              upstreamRes.data.on('error', (err) => {
                console.error('[ChatHandler] OpenAI upstream stream error', err && err.message ? err.message : err);
                try { res.end(); } catch (e) {}
              });

              // accumulate and save assistant message if possible
              let buf = '';
              upstreamRes.data.on('data', (c) => { try { buf += c.toString(); } catch {} });
              upstreamRes.data.on('end', () => {
                try {
                  // try to parse JSON if upstream returned a final JSON
                  const parsed = JSON.parse(buf);
                  const content = parsed?.choices?.[0]?.message?.content || parsed?.choices?.[0]?.text || '';
                  if (content) sessionsManager.addMessage(userId, { role: 'assistant', content }, currentConvId);
                } catch (e) { /* ignore parse errors */ }
              });
              return;
            } else {
              // Non-streaming: get JSON and return it
              const upstreamRes = await axios({
                method: 'post',
                url: upstreamUrl,
                headers: {
                  'Content-Type': 'application/json',
                  'X-NEZ-TOKEN': nezToken
                },
                data: Object.assign({}, body, { messages: ensured }),
                responseType: 'json',
                timeout: 300000
              });

              const data = upstreamRes.data || {};
              const content = data?.choices?.[0]?.message?.content || data?.choices?.[0]?.text || data?.response || '';
              if (content) {
                sessionsManager.addMessage(userId, { role: 'assistant', content }, currentConvId);
                console.log('[Session] Message assistant sauvardé (openai non-stream proxy)');
              }
              return res.status(upstreamRes.status).json(data);
            }
          } catch (err) {
            console.error('[ChatHandler] OpenAI proxy error:', err && (err.message || (err.response && JSON.stringify(err.response.data)) ) );
            if (err.response && err.response.data) {
              try { return res.status(err.response.status || 502).json(err.response.data); } catch (e) {}
            }
            return res.status(502).json({ error: 'openai_proxy_error', message: String(err && err.message) });
          }
        }

        // Special handling for llama upstream: call upstream with stream:false and return an SSE-like single chunk
        const wantsLlama = (clientProvider === 'llama') || (String(body.model || '').toLowerCase().includes('llama'));
        if (wantsLlama) {
          try {
            const upstream = (process.env.LLAMA_BASE && process.env.LLAMA_BASE.trim()) ? process.env.LLAMA_BASE.trim() : DEFAULT_UPSTREAM;
            const apiUrl = `${String(upstream).replace(/\/$/, '')}/v1/chat/completions`;
            const upstreamBody = Object.assign({}, body, { messages: ensured, stream: false });
            console.log('[ChatHandler] Llama bridge debug', { apiUrl, upstreamBodyPreview: JSON.stringify(upstreamBody).slice(0,1000) });
            const r = await axios.post(apiUrl, upstreamBody, { responseType: 'json', timeout: 300000 });
            const data = r.data || {};
            const content = data?.choices?.[0]?.message?.content || data?.choices?.[0]?.text || data?.response || '';

            // Send SSE-like response with single chunk then [DONE]
            res.setHeader('Content-Type', 'text/event-stream');
            res.setHeader('Cache-Control', 'no-cache');
            res.setHeader('Connection', 'keep-alive');
            if (res.flushHeaders) try { res.flushHeaders(); } catch (e) {}

            const chunk = {
              id: data.id || null,
              object: 'chat.completion.chunk',
              choices: [
                {
                  index: 0,
                  delta: { content },
                  finish_reason: content ? 'stop' : null
                }
              ]
            };

            try { res.write(`data: ${JSON.stringify(chunk)}\n\n`); } catch (e) {}
            try { res.write('data: [DONE]\n\n'); } catch (e) {}
            try { res.end(); } catch (e) {}

            // Persist assistant message
            if (content) {
              sessionsManager.addMessage(userId, { role: 'assistant', content }, currentConvId);
              console.log(`[Session] Message assistant sauvardé (llama non-stream -> SSE bridge)`);
            }
            return;
          } catch (err) {
            console.error('[ChatHandler] Llama bridge error:', err && err.message ? err.message : err);
            // forward upstream error body if available
            if (err.response && err.response.data) {
              try { return res.status(err.response.status || 502).json(err.response.data); } catch (e) {}
            }
            return res.status(502).json({ error: 'llama_bridge_error', message: String(err && err.message) });
          }
        }

        // Default: proxy to Llama/Ollama
        try {
          const { apiUrl, apiBody, responseType, isOllama, isStreaming } = prepareApiRequest(body, ensured);
          const r = await axios.post(apiUrl, apiBody, { responseType });
          if (isStreaming) {
            await handleStreamingResponse(r, isOllama, userId, currentConvId, res);
          } else {
            await handleNonStreamingResponse(r, isOllama, userId, currentConvId, res);
          }
        } catch (e) {
          handleChatError(e, res);
        }
      } catch (e) {
        handleChatError(e, res);
      }
    };
    // Register chat completions handler for local mode
    app.post('/v1/chat/completions', express.json({ limit: '10mb' }), chatCompletionsHandler);
    app.post('/api/v1/chat/completions', express.json({ limit: '10mb' }), chatCompletionsHandler);
  }
}

// Optionally export start for external use
module.exports = { start };
